{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Fine-tuning a SpaceNet pre-trained model with __Solaris__\n",
    "\n",
    "This notebook is developed for the FOSS4G International 2019 `solaris` Workshop. If you're using it outside of that context, some of the working environment materials will be unavailable. Check the GitHub repo for instructions on how to alter the notebooks for usage outside of the workshop.\n",
    " \n",
    "## Summary\n",
    "\n",
    "This notebook shows how to take a pre-trained SpaceNet Challenge-winning model and fine-tune it to work on a new imagery dataset. Note that this task requires a fair amount of computational oomph, and will be very slow without access to a GPU.\n",
    "\n",
    "This notebook is split into 3 parts:\n",
    "\n",
    "1. [__Checking model performance on a new dataset__](#Checking-model-performance-on-a-new-dataset)\n",
    "    1. [Checking performance on the original input imagery](#Checking-performance-on-the-original-input-imagery)\n",
    "    2. [Calculating dataset mean and standard deviation](#Calculating-dataset-mean-and-standard-deviation)\n",
    "    3. [Re-writing the YAML config file for a new experiment](#Re-writing-the-YAML-config-file-for-a-new-experiment)\n",
    "    4. [Evaluating prediction quality on Khartoum data](#Evaluating-prediction-quality-on-Khartoum-data)\n",
    "2. [__Fine-tuning the model__](#fine-tuning-the-model)\n",
    "    1. [Creating training masks](#Creating-training-masks)\n",
    "    2. [Building the config file](#Building-the-config-file)\n",
    "    3. [Model training](#Model-training)\n",
    "    4. [Predictions with the new model](#Predictions-with-the-new-model)\n",
    "3. [__Scoring model performance after fine-tuning__](#Scoring-model-performance-after-fine-tuning)\n",
    "\n",
    "## Checking model performance on a new dataset\n",
    "\n",
    "When a model is trained on imagery from one geography (or even a small set of geographies), it may not _\"generalize\"_ well, i.e. it may perform poorly on previously unseen geographies. Let's test that out with the [Khartoum AOI from the SpaceNet Dataset](https://spacenet.ai/spacenet-buildings-dataset-v2/).\n",
    "\n",
    "We'll check to see how well the model trained on Atlanta data performs when we test on this image of Khartoum, Sudan from the [SpaceNet 2: Building Footprint Extraction Challenge](https://spacenet.ai/spacenet-buildings-dataset-v2/):\n",
    "\n",
    "<img src=\"files/khartoum_infer_for_viz.png\">\n",
    "\n",
    "First, let's see how inference performs on the untouched, raw image:\n",
    "\n",
    "### Checking performance on the original input imagery\n",
    "\n",
    "We'll run inference just as we did previously, but using a config file that points to the Khartoum imagery instead of the MVOI (Atlanta) imagery. Note that we're not doing _any_ normalization here - we're just going to put the raw image in and see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import solaris as sol\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import skimage\n",
    "import geopandas as gpd\n",
    "from shapely.ops import cascaded_union  # just for visualization purposes\n",
    "\n",
    "\n",
    "data_path = '/data'   # NON-WORKSHOP PARTICIPANTS: change this path to point to the directory where you've stored the data.\n",
    "\n",
    "print('Loading config...')\n",
    "config = sol.utils.config.parse(os.path.join(data_path, 'workshop_configs/xdxd_workshop_khartoum_infer_raw.yml'))\n",
    "print('config loaded. Initializing model...')\n",
    "xdxd_inferer = sol.nets.infer.Inferer(config)\n",
    "print('model initialized. Loading dataset...')\n",
    "inf_df = sol.nets.infer.get_infer_df(config)\n",
    "print('dataset loaded. Running inference on the image.')\n",
    "start_time = time.time()\n",
    "xdxd_inferer(inf_df)\n",
    "end_time = time.time()\n",
    "print('running inference on one image took {} seconds'.format(end_time-start_time))\n",
    "print('vectorizing output...')\n",
    "resulting_preds = skimage.io.imread('xdxd_inference_out/RGB-PanSharpen_AOI_5_Khartoum_img924.tif')\n",
    "predicted_footprints = sol.vector.mask.mask_to_poly_geojson(\n",
    "    pred_arr=resulting_preds,\n",
    "    reference_im=inf_df.loc[0, 'image'],\n",
    "    do_transform=True,\n",
    "    min_area=1e-10)  # need min_area=0 since the coord system is lat/long rather than UTM (metric)\n",
    "print('output vectorized.')\n",
    "predicted_footprints.to_file('xdxd_inference_out/Khartoum_img924_raw.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_im_path = os.path.join(data_path, 'Khartoum_data/RGB_imagery/RGB-PanSharpen_AOI_5_Khartoum_img924.tif')\n",
    "# read the image in\n",
    "im_arr = skimage.io.imread(src_im_path)\n",
    "# rescale to min/max in each channel\n",
    "im_arr = im_arr.astype('float') - np.amin(im_arr, axis=(0,1))\n",
    "im_arr = im_arr/np.amax(im_arr, axis=(0,1))\n",
    "im_arr = (im_arr*255).astype('uint8')\n",
    "# generate mask from the predictions\n",
    "pred_arr = skimage.io.imread('xdxd_inference_out/RGB-PanSharpen_AOI_5_Khartoum_img924.tif')\n",
    "preds = (pred_arr[:, :, 0] > 0).astype('uint8')\n",
    "ground_truth = sol.vector.mask.footprint_mask(\n",
    "    os.path.join(data_path, 'Khartoum_data/geojson/buildings_AOI_5_Khartoum_img924.geojson'),\n",
    "    reference_im=src_im_path)\n",
    "\n",
    "f, axarr = plt.subplots(1, 3, figsize=(16,12))\n",
    "axarr[0].imshow(im_arr)\n",
    "axarr[0].set_title('Source image', size=14)\n",
    "axarr[1].imshow(preds, cmap='gray')\n",
    "axarr[1].set_title('Predictions', size=14)\n",
    "axarr[2].imshow(ground_truth, cmap='gray')\n",
    "axarr[2].set_title('Ground Truth', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly those results are garbage.\n",
    "\n",
    "This shows how important it is to make sure your inference target imagery is _normalized the same way your training data was_ when you pass it into a neural network. If it's not, the network has no idea what to do with the values it sees in the array!\n",
    "\n",
    "When you passed imagery from MVOI into the neural net, it was _Z-scored_ - that is, the mean of the pixel intensities was set to zero and each band's standard deviation was set to 1. By contrast, the Khartoum image that you just fed in was 16-bit integers. This explains why the model performed poorly!\n",
    "\n",
    "Next we'll go through how to Z-score the Khartoum imagery, then re-do the inference with that normalization.\n",
    "\n",
    "### Calculating dataset mean and standard deviation\n",
    "First, it's important to ensure that the data from Khartoum is normalized the same way as the data from Atlanta, as differences in intensity will propogate through the entire network, disrupting model performance. The Atlanta data is Z-scored, so we will do the same for Khartoum; to this end, we need to calculate the mean and standard deviation for each channel in the Khartoum dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = [f for f in os.listdir(os.path.join(data_path, 'Khartoum_data/RGB_imagery'))]\n",
    "\n",
    "R_cts = np.zeros(shape=(199,), dtype='uint32')\n",
    "G_cts = np.zeros(shape=(199,), dtype='uint32')\n",
    "B_cts = np.zeros(shape=(199,), dtype='uint32')\n",
    "bins = np.arange(0, 2000, 10)\n",
    "\n",
    "for idx, im in enumerate(ims):\n",
    "    curr_im = sol.utils.io.imread(os.path.join(data_path, 'Khartoum_data', 'RGB_imagery', im))\n",
    "    R_cts += np.array(np.histogram(curr_im[:, :, 0], bins=bins)[0], dtype='uint32')\n",
    "    G_cts += np.array(np.histogram(curr_im[:, :, 1], bins=bins)[0], dtype='uint32')\n",
    "    B_cts += np.array(np.histogram(curr_im[:, :, 2], bins=bins)[0], dtype='uint32')\n",
    "    if idx%100 == 0:\n",
    "        print(\"# {} of {} completed\".format(idx, len(ims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The above cell takes a couple of minutes - be patient!_\n",
    "\n",
    "Let's look at the histogram of values for the channels. We're going to skip the first bin, as this is almost exclusively made up of `0` values. Zeros correspond to no data in these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.plot(bins[1:-1] + 5, R_cts[1:], label='Red', color='red')\n",
    "ax.plot(bins[1:-1] + 5, G_cts[1:], label='Green', color='green')\n",
    "ax.plot(bins[1:-1] + 5, B_cts[1:], label='Blue', color='blue')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlabel('Intensity value', size=16)\n",
    "ax.set_ylabel('Counts', size=16)\n",
    "ax.set_title('Counts of pixel intensities for Khartoum data,\\nsplit by channel',\n",
    "             size=16);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we'll calculate the mean and standard deviation to normalize these intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std_from_histogram(bins, cts):\n",
    "    \"\"\"Calculate the mean and standard deviation from a histogram.\"\"\"\n",
    "    bin_centers = bins[1:-1] + ((bins[1]-bins[0])/2.)\n",
    "    # skip the first bin since it contains the nodata values\n",
    "    mean = np.sum(cts[1:]*bin_centers)/np.sum(cts[1:])\n",
    "    std = np.sqrt((1./sum(cts[1:]))*np.sum(cts[1:]*np.square(bin_centers-mean)))\n",
    "    return mean, std\n",
    "\n",
    "r_mean, r_std = mean_and_std_from_histogram(bins, R_cts)\n",
    "print(\"Red mean: {}\".format(r_mean))\n",
    "print(\"Red standard deviation: {}\".format(r_std))\n",
    "g_mean, g_std = mean_and_std_from_histogram(bins, G_cts)\n",
    "print(\"Green mean: {}\".format(g_mean))\n",
    "print(\"Green standard deviation: {}\".format(g_std))\n",
    "b_mean, b_std = mean_and_std_from_histogram(bins, B_cts)\n",
    "print(\"Blue mean: {}\".format(b_mean))\n",
    "print(\"Blue standard deviation: {}\".format(b_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the [`albumentations`](https://albumentations.readthedocs.io/en/latest/index.html) library used in `solaris`  divides pixel intensity by the bit depth before performing normalization, we need to divide these by 65535 (the unsigned 16-bit max) for use as parameters in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"r_mean for config file: {}\".format(r_mean/65535))\n",
    "print(\"g_mean for config file: {}\".format(g_mean/65535))\n",
    "print(\"b_mean for config file: {}\".format(b_mean/65535))\n",
    "print(\"r_std for config file: {}\".format(r_std/65535))\n",
    "print(\"g_std for config file: {}\".format(g_std/65535))\n",
    "print(\"b_std for config file: {}\".format(b_std/65535))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values (to a few decimal places) should be used in the config file.\n",
    "\n",
    "### Re-writing the YAML config file for a new experiment\n",
    "\n",
    "There are three other changes that need to be made to the original config files:\n",
    "1. Remove the `DropChannel` pre-processing step: unlike the Atlanta dataset, these image files only have three channels. We therefore don't need to drop a 4th channel.\n",
    "2. `SwapChannels`: The MVOI Atlanta dataset is B-G-R channel order, but Khartoum is R-G-B. We therefore need to use the `SwapChannels` pre-processing step to switch the channels at index `0` and `2`. Because we're using PyTorch models, these channels will be at axis 1.\n",
    "3. `inference_data_csv`: because we're fine-tuning on different training data, we'll need to point to a CSV specifying different data. That CSV, `khartoum_inf.csv`, can be found in the `workshop_configs` directory.\n",
    "\n",
    "Feel free to try to create this yourself from a copy of the `xdxd_workshop_infer.yml` file. Alternatively, we've provided the file for you as `xdxd_workshop_khartoum_infer.yml`.\n",
    "\n",
    "__Let's try it out!__\n",
    "\n",
    "### Evaluating prediction quality on Khartoum data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading config...')\n",
    "config = sol.utils.config.parse(os.path.join(data_path, 'workshop_configs/xdxd_workshop_khartoum_infer.yml'))\n",
    "print('config loaded. Initializing model...')\n",
    "xdxd_inferer = sol.nets.infer.Inferer(config)\n",
    "print('model initialized. Loading dataset...')\n",
    "inf_df = sol.nets.infer.get_infer_df(config)\n",
    "print('dataset loaded. Running inference on the image.')\n",
    "start_time = time.time()\n",
    "xdxd_inferer(inf_df)\n",
    "end_time = time.time()\n",
    "print('running inference on one image took {} seconds'.format(end_time-start_time))\n",
    "print('vectorizing output...')\n",
    "resulting_preds = skimage.io.imread('xdxd_inference_out/RGB-PanSharpen_AOI_5_Khartoum_img924.tif')\n",
    "predicted_footprints = sol.vector.mask.mask_to_poly_geojson(\n",
    "    pred_arr=resulting_preds,\n",
    "    reference_im=inf_df.loc[0, 'image'],\n",
    "    do_transform=True,\n",
    "    min_area=1e-10)  # need min_area=0 since the coord system is lat/long rather than UTM (metric)\n",
    "print('output vectorized.')\n",
    "predicted_footprints.to_file('xdxd_inference_out/Khartoum_img924.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_im_path = os.path.join(data_path, 'Khartoum_data/RGB_imagery/RGB-PanSharpen_AOI_5_Khartoum_img924.tif')\n",
    "# read the image in\n",
    "im_arr = skimage.io.imread(src_im_path)\n",
    "# rescale to min/max in each channel\n",
    "im_arr = im_arr.astype('float') - np.amin(im_arr, axis=(0,1))\n",
    "im_arr = im_arr/np.amax(im_arr, axis=(0,1))\n",
    "im_arr = (im_arr*255).astype('uint8')\n",
    "# generate mask from the predictions\n",
    "pred_arr = skimage.io.imread('xdxd_inference_out/RGB-PanSharpen_AOI_5_Khartoum_img924.tif')\n",
    "preds = (pred_arr[:, :, 0] > 0).astype('uint8')\n",
    "ground_truth = sol.vector.mask.footprint_mask(\n",
    "    os.path.join(data_path, 'Khartoum_data/geojson/buildings_AOI_5_Khartoum_img924.geojson'),\n",
    "    reference_im=src_im_path)\n",
    "\n",
    "f, axarr = plt.subplots(1, 3, figsize=(16,12))\n",
    "axarr[0].imshow(im_arr)\n",
    "axarr[0].set_title('Source image', size=14)\n",
    "axarr[1].imshow(preds, cmap='gray')\n",
    "axarr[1].set_title('Predictions', size=14)\n",
    "axarr[2].imshow(ground_truth, cmap='gray')\n",
    "axarr[2].set_title('Ground Truth', size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(12,4))\n",
    "axarr[0].imshow(pred_arr[:, :, 0], cmap='gray')\n",
    "axarr[0].axis('off')\n",
    "axarr[0].set_title('Raw predictions', size=16)\n",
    "axarr[1].hist(pred_arr.flatten(), bins=25, density=True)\n",
    "axarr[1].set_xlabel('Raw confidence', size=14)\n",
    "axarr[1].set_ylabel('Fraction of pixels', size=14)\n",
    "axarr[1].set_title('Prediction histogram', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions are clearly terrible - the model is only finding one of the buildings in this image. However, if we directly examine the prediction outputs, we'll see that we're not _too_ far from a good model - it's finding some buildings, just at such a low raw confidence value that it can't distinguish them from background. Remember that the model takes anything with a raw prediction > 0 as a building.\n",
    "\n",
    "__Pause here to go through the CosmiQ_Solaris_Training_Intro slides!__\n",
    "\n",
    "So, what can we do to improve model performance? Let's try fine-tuning!\n",
    "\n",
    "## Fine-tuning the model\n",
    "\n",
    "### Creating training masks\n",
    "Before we can continue training a model, we need target masks: images that the model will learn to create during training. We'll follow [this tutorial](https://solaris.readthedocs.io/en/latest/tutorials/notebooks/api_masks_tutorial.html) to create masks. __Note for workshop participants:__ this cell won't work because the `/data` directory is read-only; we've made the training masks for you, but this cell shows how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dir = os.path.join(data_path, 'Khartoum_data', 'training_masks')\n",
    "geojson_dir = os.path.join(data_path, 'Khartoum_data', 'geojson')\n",
    "im_dir = os.path.join(data_path, 'Khartoum_data', 'RGB_imagery')\n",
    "geojson_list = [f for f in os.listdir(geojson_dir) if f.endswith('.geojson')]\n",
    "im_list = [f for f in os.listdir(geojson_dir) if f.endswith('.tif')]\n",
    "n_chips = len(geojson_list)\n",
    "\n",
    "if not os.path.exists(mask_dir):\n",
    "    os.mkdir(mask_dir)\n",
    "    \n",
    "    for idx, gj in enumerate(geojson_list):\n",
    "        # get the 'img[number] chip ID for the image'\n",
    "        chip_id = os.path.splitext(gj)[0].split('_')[-1]\n",
    "        matching_im = 'RGB-PanSharpen_AOI_5_Khartoum_' + chip_id + '.tif'\n",
    "        mask_fname = 'mask_' + chip_id + '.tif'\n",
    "        fp_mask = sol.vector.mask.footprint_mask(df=os.path.join(geojson_dir, gj),\n",
    "                                                 out_file=os.path.join(mask_dir, mask_fname),\n",
    "                                                 reference_im=os.path.join(im_dir, matching_im),\n",
    "                                                 shape=(650, 650))\n",
    "        if (idx+1)%100 == 0:\n",
    "            print('chip {} of {} done'.format(idx+1, n_chips), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of these just to make sure they came out right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(10, 6))\n",
    "axarr[0].imshow(skimage.io.imread('files/khartoum_infer_for_viz.tif'))\n",
    "axarr[0].axis('off')\n",
    "axarr[1].imshow(skimage.io.imread(os.path.join(data_path, 'Khartoum_data', 'training_masks', 'mask_img924.tif')),\n",
    "                cmap='gray')\n",
    "axarr[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! We're ready to set up for training.\n",
    "\n",
    "### Building the config file\n",
    "\n",
    "With model fine-tuning, we'll load the pre-trained weights used above, and continue training at a much lower learning rate for a couple of epochs. To this end we'll need _another_ config with a few more modifications:\n",
    "\n",
    "1. A reduced learning rate - we'll try `1e-5` instead of `1e-4`\n",
    "2. Change `train=False` to `train=True`\n",
    "3. Specify where the newly trained versions are saved with the `training['callbacks']['model_checkpoint']` arguments\n",
    "4. Specify a training data CSV. In this case, we'll use a CSV created [per this tutorial](https://solaris.readthedocs.io/en/latest/tutorials/notebooks/creating_im_reference_csvs.html) that points to all of the images and the masks that we just created, save for one: the image that we inferenced against earlier, which we'll save as a test image. The csv, named `khartoum_fine_tune.csv`, is available in the `workshop_configs` directory.\n",
    "\n",
    "As earlier, feel free to create this config yourself; otherwise, you can use `xdxd_workshop_khartoum_train.yml`.\n",
    "\n",
    "### Model training\n",
    "\n",
    "Let's try it! <font style=\"color: red;\">__WARNING: this is EXTREMELY slow without a GPU (each epoch may take several hours).__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading config...')\n",
    "config = sol.utils.config.parse(os.path.join(data_path, 'workshop_configs/xdxd_workshop_khartoum_train.yml'))\n",
    "print('config loaded. Initializing Trainer instance...')\n",
    "xdxd_trainer = sol.nets.train.Trainer(config)\n",
    "print('model initialized. Beginning training...')\n",
    "print()\n",
    "start_time = time.time()\n",
    "xdxd_trainer.train()\n",
    "end_time = time.time()\n",
    "print()\n",
    "print('training took {} minutes'.format((end_time-start_time)/60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with the new model\n",
    "\n",
    "We'll now run inference with the newly tuned model. Note that if your config file specifies `train=True` and you pass that config to an `Inferer` instance, `solaris` will automatically use the newly trained model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading config...')\n",
    "config = sol.utils.config.parse(os.path.join(data_path, 'workshop_configs/xdxd_workshop_khartoum_train.yml'))\n",
    "print('config loaded. Initializing model...')\n",
    "xdxd_inferer = sol.nets.infer.Inferer(config)\n",
    "print('model initialized. Loading dataset...')\n",
    "inf_df = sol.nets.infer.get_infer_df(config)\n",
    "print('dataset loaded. Running inference on the image.')\n",
    "start_time = time.time()\n",
    "xdxd_inferer(inf_df)\n",
    "end_time = time.time()\n",
    "print('running inference on one image took {} seconds'.format(end_time-start_time))\n",
    "print('vectorizing output...')\n",
    "resulting_preds = skimage.io.imread('xdxd_retrain_inference_out/RGB-PanSharpen_AOI_5_Khartoum_img924.tif')\n",
    "predicted_footprints = sol.vector.mask.mask_to_poly_geojson(\n",
    "    pred_arr=resulting_preds,\n",
    "    reference_im=inf_df.loc[0, 'image'],\n",
    "    do_transform=True,\n",
    "    min_area=1e-10)  # need min_area=0 since the coord system is lat/long rather than UTM (metric)\n",
    "print('output vectorized.')\n",
    "predicted_footprints.to_file('xdxd_retrain_inference_out/Khartoum_img924.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_im_path = os.path.join(data_path, 'Khartoum_data/RGB_imagery/RGB-PanSharpen_AOI_5_Khartoum_img924.tif')\n",
    "# read the image in\n",
    "im_arr = skimage.io.imread(src_im_path)\n",
    "# rescale to min/max in each channel\n",
    "im_arr = im_arr.astype('float') - np.amin(im_arr, axis=(0,1))\n",
    "im_arr = im_arr/np.amax(im_arr, axis=(0,1))\n",
    "im_arr = (im_arr*255).astype('uint8')\n",
    "# generate mask from the predictions\n",
    "old_pred_arr = skimage.io.imread('xdxd_inference_out/RGB-PanSharpen_AOI_5_Khartoum_img924.tif')\n",
    "old_preds = (old_pred_arr[:, :, 0] > 0).astype('uint8')\n",
    "new_pred_arr = skimage.io.imread('xdxd_retrain_inference_out/RGB-PanSharpen_AOI_5_Khartoum_img924.tif')\n",
    "new_preds = (new_pred_arr[:, :, 0] > 0).astype('uint8')\n",
    "\n",
    "ground_truth = sol.vector.mask.footprint_mask(\n",
    "    os.path.join(data_path, 'Khartoum_data/geojson/buildings_AOI_5_Khartoum_img924.geojson'),\n",
    "    reference_im=src_im_path)\n",
    "\n",
    "f, axarr = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axarr[0, 0].imshow(im_arr)\n",
    "axarr[0, 0].set_title('Source image', size=14)\n",
    "axarr[0, 0].axis('off')\n",
    "axarr[0, 1].imshow(old_preds, cmap='gray')\n",
    "axarr[0, 1].set_title('Predictions before fine-tuning', size=14)\n",
    "axarr[0, 1].axis('off')\n",
    "axarr[1, 1].imshow(new_preds, cmap='gray')\n",
    "axarr[1, 1].set_title('Predictions after fine-tuning', size=14)\n",
    "axarr[1, 1].axis('off')\n",
    "axarr[1, 0].imshow(ground_truth, cmap='gray')\n",
    "axarr[1, 0].set_title('Ground Truth', size=14)\n",
    "axarr[1, 0].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. This appears to show a _marked_ improvement with _just two epochs of training!_ How do the scores come out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring model performance after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = sol.eval.base.Evaluator(os.path.join(data_path, 'Khartoum_data/geojson/buildings_AOI_5_Khartoum_img924.geojson'))\n",
    "prediction_dirs = ['xdxd_inference_out', 'xdxd_retrain_inference_out']\n",
    "model_names = ['Original', 'Fine-tuned']\n",
    "\n",
    "f1_scores = []\n",
    "precision = []\n",
    "recall = []\n",
    "for i in range(2):\n",
    "    evaluator.load_proposal(os.path.join(prediction_dirs[i],'Khartoum_img924.geojson'),\n",
    "                            pred_row_geo_value='geometry',\n",
    "                            conf_field_list=[])\n",
    "    results = evaluator.eval_iou(miniou=0.5, calculate_class_scores=False)\n",
    "    f1_scores.append(results[0]['F1Score'])\n",
    "    precision.append(results[0]['Precision'])\n",
    "    recall.append(results[0]['Recall'])\n",
    "\n",
    "f, axarr = plt.subplots(1, 3, figsize=(10, 4))\n",
    "f.subplots_adjust(wspace=0.6)\n",
    "axarr[0].bar(model_names, f1_scores)\n",
    "axarr[0].set_ylabel('$F_1$ Score', size=16)\n",
    "axarr[1].bar(model_names, precision)\n",
    "axarr[1].set_ylabel('Precision', size=16)\n",
    "axarr[2].bar(model_names, recall)\n",
    "axarr[2].set_ylabel('Recall', size=16);\n",
    "f.suptitle('Comparison of original vs. fine-tuned model performance', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this is only _one_ sample image; however, it's noteworthy that this model briefly fine-tuned on Khartoum imagery [__achieved a higher score here than some of the prize-winning models trained on Khartoum for days during the SpaceNet Challenge Round 2__](https://medium.com/the-downlinq/2nd-spacenet-competition-winners-code-release-c7473eea7c11).\n",
    "\n",
    "# Congratulations! You've completed the FOSS4G 2019 Solaris tutorial.\n",
    "\n",
    "Hang around for a quick teaser on the SpaceNet 5 challenge that's starting soon!\n",
    "\n",
    "## What's next?\n",
    "\n",
    "Here are a few more resources that will help you as you continue to work with `solaris`:\n",
    "\n",
    "- [Solaris documentation](https://solaris.readthedocs.io)\n",
    "- [A blog post from Jake Shermeyer about using Solaris for car detection in the COWC dataset](https://medium.com/the-downlinq/beyond-infrastructure-mapping-finding-vehicles-with-solaris-11e08da0dab)\n",
    "- [The Solaris GitHub repository](https://github.com/cosmiq/solaris)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solaris",
   "language": "python",
   "name": "solaris"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
