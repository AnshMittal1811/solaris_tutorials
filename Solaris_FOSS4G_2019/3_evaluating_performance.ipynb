{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Evaluating prediction quality with __Solaris__\n",
    "\n",
    "Here, we're going to try running several different pre-trained models to generate predictions on the exact same image to help us decide which one works best.\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. [__Running inference with the other pre-trained models__](#Running-inference-with-the-other-pre-trained-models)\n",
    "2. [__Pixel-wise metrics for prediction quality__](#Pixel-wise-metrics-for-prediction-quality)\n",
    "3. [__Object-wise metrics for prediction quality__](#Object-wise-metrics-for-prediction-quality)\n",
    "\n",
    "While we talk through evaluation metrics, it'll be helpful to have a few different models' outputs to compare. To this end, we're going to run inference with all four of the pre-trained models available in `solaris` on the sample image we've used previously. The following cell runs that inference pipeline for the other three models, just like we did for XD_XD's model earlier; let's get it started!\n",
    "\n",
    "## Running inference with the other pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import solaris as sol\n",
    "import numpy as np\n",
    "import skimage\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "data_path = '/data'  # NON-WORKSHOP PARTICIPANTS: change this path to point to the directory where you've stored the data.\n",
    "\n",
    "def run_inference(config_path):\n",
    "    print('Loading config...')\n",
    "    config = sol.utils.config.parse(config_path)\n",
    "    print('Config loaded. Initializing model...')\n",
    "    inferer = sol.nets.infer.Inferer(config)\n",
    "    print('Model initialized. Loading dataset...')\n",
    "    inf_df = sol.nets.infer.get_infer_df(config)\n",
    "    print('Dataset loaded. Running inference on the image...this could take a bit, be patient.')\n",
    "    start_time = time.time()\n",
    "    inferer(inf_df)\n",
    "    end_time = time.time()\n",
    "    print('Running inference on one image took {} seconds.'.format(end_time-start_time))\n",
    "    print('Vectorizing output...')\n",
    "    resulting_preds = skimage.io.imread(os.path.join(config['inference']['output_dir'], 'MVOI_nadir10_test_sample.tif'))\n",
    "    # handle issue we currently have with multi-channel masks and automated footprint generation\n",
    "    if config['data_specs']['mask_channels'] > 1:\n",
    "        resulting_preds = resulting_preds[:, :, 0]\n",
    "    predicted_footprints = sol.vector.mask.mask_to_poly_geojson(\n",
    "        pred_arr=resulting_preds,\n",
    "        reference_im=os.path.join(data_path, 'MVOI_data/MVOI_nadir10_test_sample.tif'),\n",
    "        do_transform=True)\n",
    "    print('Output vectorized.')\n",
    "    predicted_footprints.to_file(os.path.join(config['inference']['output_dir'], 'MVOI_nadir10_predictions.geojson'), driver='GeoJSON')\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config_path in ('workshop_configs/selimsef_densenet121unet_workshop_infer.yml',\n",
    "                    'workshop_configs/selimsef_densenet161unet_workshop_infer.yml',\n",
    "                    'workshop_configs/selimsef_resnet34unet_workshop_infer.yml',\n",
    "                    'workshop_configs/xdxd_workshop_infer.yml'):\n",
    "    run_inference(os.path.join(data_path, config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the above cell finishes, we'll visualize the outputs.\n",
    "\n",
    "#### Visualizing the outputs\n",
    "\n",
    "Though it's preferable to work with vector-formatted labels, the raster versions are generally easier to visualize with `matplotlib`, so we'll show those here for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dirs = ['xdxd_inference_out', 'selimsef_resnet34_inference_out',\n",
    "               'selimsef_densenet121_inference_out', 'selimsef_densenet161_inference_out']\n",
    "model_names = ['VGG16', 'ResNet34', 'DenseNet121', 'DenseNet161']\n",
    "\n",
    "f, axarr = plt.subplots(3, 2, figsize=(12, 20))\n",
    "\n",
    "for i in range(4):\n",
    "    im = skimage.io.imread(os.path.join(output_dirs[i], 'MVOI_nadir10_test_sample.tif'))\n",
    "    im = (im > 0).astype('uint8')[:, :, 0]\n",
    "    axarr[i//2, i%2].imshow(im, cmap='gray')\n",
    "    axarr[i//2, i%2].set_title(model_names[i], size=16)\n",
    "    axarr[i//2, i%2].axis('off')\n",
    "\n",
    "ground_truth = sol.vector.mask.footprint_mask(\n",
    "    os.path.join(data_path, 'MVOI_data/MVOI_nadir10_test_sample.geojson'),\n",
    "    reference_im = os.path.join(data_path, 'MVOI_data/MVOI_nadir10_test_sample.tif'))\n",
    "\n",
    "axarr[2, 0].imshow(ground_truth, cmap='gray')\n",
    "axarr[2, 0].set_title('Ground Truth labels', size=16)\n",
    "axarr[2, 0].axis('off')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# switch B and R for viz\n",
    "im_arr = skimage.io.imread(os.path.join(data_path, 'MVOI_data/viz_version.tif'))\n",
    "axarr[2, 1].imshow(im_arr[:, :, 0:3])\n",
    "axarr[2, 1].set_title('Source image', size=16)\n",
    "axarr[2, 1].axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the models are _generally_ very similar in their predictions, but there are some subtle differences. Some of these may have dramatic effects on how their performance is scored! But how would we go about deciding which is best?\n",
    "\n",
    "__Pause here for slides: CosmiQ_Solaris_Evaluating_Performance__\n",
    "\n",
    "\n",
    "## Pixel-wise metrics for prediction quality\n",
    "\n",
    "We'll start by evaluating model performance with a couple of pixel-wise metrics, i.e., metrics that score models based on what fraction of the pixels are correct.\n",
    "\n",
    "Let's start with \"accuracy\", since that's the most common term used when discussing model performance. Few people are actually familiar wtih the definition of accuracy:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "accuracy=\\frac{TN+TP}{TN+TP+FN+FP}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- TN = True Negative (a pixel is ID'ed as non-building, and is in fact not a building)\n",
    "- TP = True Positive (a pixel is ID'ed as building, and is in fact a building)\n",
    "- FN = False Negative (a pixel is ID'ed as non-building, when it is, in fact, building)\n",
    "- FP = False Positive (a pixel is ID'ed as building, but it's actually background)\n",
    "\n",
    "In layperson's terms, this metric is essentially what fraction of pixels are correct. Let's calculate that for each model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(prediction, ground_truth):\n",
    "    return np.sum(prediction == ground_truth)/float(prediction.size)\n",
    "\n",
    "output_dirs = ['xdxd_inference_out', 'selimsef_resnet34_inference_out',\n",
    "               'selimsef_densenet121_inference_out', 'selimsef_densenet161_inference_out']\n",
    "model_names = ['VGG16\\nU-Net', 'ResNet34\\nU-Net', 'DenseNet121\\nU-Net', 'DenseNet161\\nU-Net']\n",
    "\n",
    "ground_truth = ground_truth > 0\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for i in range(4):\n",
    "    pred_image = skimage.io.imread(os.path.join(output_dirs[i], 'MVOI_nadir10_test_sample.tif'))\n",
    "    pred_image = pred_image > 0\n",
    "    pred_image = pred_image[:, :, 0]\n",
    "    accuracy = acc(pred_image, ground_truth)\n",
    "    print(\"{} accuracy: {}\".format(model_names[i], accuracy))\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "plt.bar(model_names, accuracy_scores, color='black')\n",
    "plt.ylabel('Accuracy', size=16)\n",
    "plt.ylim((0.85, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are _very_ similar - tiny fractions of a percent different - and more or less impossible to detect, even zoomed way in on the graph (note the Y axis on the graph above). What if we try a different metric, like $F_1$ score:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "F_1 = 2\\times\\frac{P\\times R}{P+R}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Where\n",
    "- P = Precision:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{TP}{TP+FP}\n",
    "\\end{align}\n",
    "$\n",
    "- R = Recall:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{TP}{TP+FN}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "precision = []\n",
    "recall = []\n",
    "for i in range(4):\n",
    "    pred_image = skimage.io.imread(os.path.join(output_dirs[i], 'MVOI_nadir10_test_sample.tif'))\n",
    "    pred_image = pred_image > 0\n",
    "    pred_image = pred_image[:, :, 0]\n",
    "    results = sol.eval.pixel.f1(ground_truth, pred_image)\n",
    "#    print(\"{} F1: {}\".format(model_names[i], results[0]))\n",
    "#    print(\"{} Precision: {}\".format(model_names[i], results[1]))\n",
    "#    print(\"{} Recall: {}\".format(model_names[i], results[2]))\n",
    "    f1_scores.append(results[0])\n",
    "    precision.append(results[1])\n",
    "    recall.append(results[2])\n",
    "\n",
    "f, axarr = plt.subplots(1, 3, figsize=(10, 4))\n",
    "f.subplots_adjust(wspace=0.6)\n",
    "axarr[0].bar(model_names, f1_scores)\n",
    "axarr[0].set_xticklabels(labels=model_names, rotation=90)\n",
    "axarr[0].set_ylabel('$F_1$ Score', size=16)\n",
    "axarr[1].bar(model_names, precision)\n",
    "axarr[1].set_xticklabels(labels=model_names, rotation=90)\n",
    "axarr[1].set_ylabel('Precision', size=16)\n",
    "axarr[2].bar(model_names, recall)\n",
    "axarr[2].set_xticklabels(labels=model_names, rotation=90)\n",
    "axarr[2].set_ylabel('Recall', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows _some_ pixel-wise difference between these different predictions - specifically, the VGG16 model provides _slightly_ better prediction quality on a pixel-by-pixel basis. The Precision and Recall plots shed a bit more light.\n",
    "\n",
    "This still leaves potential for problems, though. The two examples below produce the exact same $F_1$ score, Precision, and Recall on a pixel-wise basis:\n",
    "\n",
    "<img src='files/f1_pixelwise_prec_rec_example.png' style='width:800px'>\n",
    "\n",
    "Depending on your use case, you may want to be able to distinguish between these two sets of predictions and optimize for one rather than the other. To do so, we'll need something more than pixel-wise metrics.\n",
    "\n",
    "## Object-wise metrics for prediction quality\n",
    "At CosmiQ Works, we generally use object-wise metrics to evaluate model performance for building footprint extraction tasks (or other object identification tasks).\n",
    "This involves a two-step process, adapted from the [ImageNet](www.image-net.org) competitions:\n",
    "\n",
    "1. Call positive and negative building identifications using an Intersection-over-Union (IoU) threshold\n",
    "2. Calculate the $F_1$ score of identifications at an object level (rather than a pixel level)\n",
    "\n",
    "The graphic below displays how we calculate IoU for a prediction.\n",
    "\n",
    "<img src=\"files/iou_schematic.png\" style=\"width: 800px\">\n",
    "\n",
    "`solaris` provides functionality to do this calculation for all of the objects within a given set of vector labels, providing both the object-by-object IoU scores.\n",
    "\n",
    "Next, we set a threshold for how high a prediction's IoU must be with a ground truth object to be called \"successful\" (i.e. a True Positive). We generally set this threshold at __0.5__ for the SpaceNet Challenges. This allows us to identify specific predictions as True Positives and False Positives, as well as identify False Negatives where there wasn't a high-quality prediction. We call this metric the __SpaceNet Metric__.\n",
    "\n",
    "Using that threshold, let's assess the SpaceNet Metric for the four prediction sets we generated earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = sol.eval.base.Evaluator(os.path.join(data_path, 'MVOI_data/MVOI_nadir10_test_sample.geojson'))\n",
    "prediction_dirs = ['xdxd_inference_out', 'selimsef_resnet34_inference_out',\n",
    "               'selimsef_densenet121_inference_out', 'selimsef_densenet161_inference_out']\n",
    "model_names = ['VGG16\\nU-Net', 'ResNet34\\nU-Net', 'DenseNet121\\nU-Net', 'DenseNet161\\nU-Net']\n",
    "\n",
    "f1_scores = []\n",
    "precision = []\n",
    "recall = []\n",
    "for i in range(4):\n",
    "    evaluator.load_proposal(os.path.join(prediction_dirs[i],'MVOI_nadir10_predictions.geojson'),\n",
    "                            pred_row_geo_value='geometry',\n",
    "                            conf_field_list=[])\n",
    "    results = evaluator.eval_iou(miniou=0.5, calculate_class_scores=False)\n",
    "    f1_scores.append(results[0]['F1Score'])\n",
    "    precision.append(results[0]['Precision'])\n",
    "    recall.append(results[0]['Recall'])\n",
    "\n",
    "f, axarr = plt.subplots(1, 3, figsize=(10, 4))\n",
    "f.subplots_adjust(wspace=0.6)\n",
    "axarr[0].bar(model_names, f1_scores)\n",
    "axarr[0].set_xticklabels(labels=model_names, rotation=90)\n",
    "axarr[0].set_ylabel('$F_1$ Score', size=16)\n",
    "axarr[1].bar(model_names, precision)\n",
    "axarr[1].set_xticklabels(labels=model_names, rotation=90)\n",
    "axarr[1].set_ylabel('Precision', size=16)\n",
    "axarr[2].bar(model_names, recall)\n",
    "axarr[2].set_xticklabels(labels=model_names, rotation=90)\n",
    "axarr[2].set_ylabel('Recall', size=16);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are still close. What if we increase the IoU threshold to 0.85, meaning predictions must be much more similar to the ground truth to be deemed \"correct\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "precision = []\n",
    "recall = []\n",
    "for i in range(4):\n",
    "    evaluator.load_proposal(os.path.join(prediction_dirs[i],'MVOI_nadir10_predictions.geojson'),\n",
    "                            pred_row_geo_value='geometry',\n",
    "                            conf_field_list=[])\n",
    "    results = evaluator.eval_iou(miniou=0.85, calculate_class_scores=False)\n",
    "    f1_scores.append(results[0]['F1Score'])\n",
    "    precision.append(results[0]['Precision'])\n",
    "    recall.append(results[0]['Recall'])\n",
    "\n",
    "f, axarr = plt.subplots(1, 3, figsize=(10, 4))\n",
    "f.subplots_adjust(wspace=0.6)\n",
    "axarr[0].bar(model_names, f1_scores)\n",
    "axarr[0].set_xticklabels(labels=model_names, rotation=90)\n",
    "axarr[0].set_ylabel('$F_1$ Score', size=16)\n",
    "axarr[1].bar(model_names, precision)\n",
    "axarr[1].set_xticklabels(labels=model_names, rotation=90)\n",
    "axarr[1].set_ylabel('Precision', size=16)\n",
    "axarr[2].bar(model_names, recall)\n",
    "axarr[2].set_xticklabels(labels=model_names, rotation=90)\n",
    "axarr[2].set_ylabel('Recall', size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's notable that 1. all of the scores drop substantially, but 2. the VGG16 model really jumps out as better than the others when the threshold is increased! There's still a lot of room for improvement in this field, and we hope some of you will use these tools to make this happen.\n",
    "\n",
    "__Coming up next:__ How well do pre-trained models work on images of geographies that they've never seen before, and fine-tuning models to improve that performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solaris",
   "language": "python",
   "name": "solaris"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
